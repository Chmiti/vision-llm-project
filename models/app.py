import os
import json
import torch
import faiss
import numpy as np
from PIL import Image
import gradio as gr
from dotenv import load_dotenv
from transformers import (
    BlipProcessor, BlipForConditionalGeneration,
    CLIPProcessor, CLIPModel
)
from openai import OpenAI

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# ğŸ“¦ Charger la clÃ© API OpenAI depuis .env
load_dotenv()
client = OpenAI()  # utilisera automatiquement OPENAI_API_KEY

# ğŸ”§ Initialisation des modÃ¨les
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ğŸ§  BLIP (image â†’ caption)
blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(device)

# ğŸ§  CLIP (image â†’ embedding)
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# ğŸ“‚ Charger lâ€™index FAISS et les descriptions
faiss_index = faiss.read_index("faiss_index.index")
with open("faiss_metadata.json", "r", encoding="utf-8") as f:
    clip_metadata = json.load(f)

# ğŸš€ Pipeline principal
def pipeline(image_pil):
    try:
        # 1. ğŸ–¼ GÃ©nÃ©ration de la caption (BLIP)
        image_pil = image_pil.convert("RGB")
        inputs = blip_processor(images=image_pil, return_tensors="pt").to(device)
        caption_ids = blip_model.generate(**inputs)
        caption = blip_processor.decode(caption_ids[0], skip_special_tokens=True)

        # 2. ğŸ” Embedding avec CLIP
        clip_inputs = clip_processor(images=image_pil, return_tensors="pt").to(device)
        with torch.no_grad():
            image_feat = clip_model.get_image_features(**clip_inputs)
            image_feat = image_feat / image_feat.norm(p=2, dim=-1, keepdim=True)
        query = image_feat.cpu().numpy()

        # 3. ğŸ” Recherche dans FAISS
        D, I = faiss_index.search(query, k=5)
        similar_descriptions = [clip_metadata[i]["description"] for i in I[0]]

        # 4. ğŸ¤– Prompt GPT
        prompt = f"""Voici un panneau routier dÃ©tectÃ©.

Caption automatique : "{caption}"

Panneaux similaires dans la base :
{chr(10).join(f"- {desc}" for desc in similar_descriptions)}

ğŸ‘‰ Que signifie probablement ce panneau ? RÃ©ponds de faÃ§on claire et concise."""

        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}]
        )
        answer = response.choices[0].message.content.strip()

        # âœ… RÃ©sultat formatÃ©
        return caption, "\n".join(["- " + d for d in similar_descriptions]), answer

    except Exception as e:
        import traceback
        return "Erreur", "Erreur", f"âŒ Exception :\n{traceback.format_exc()}"

# ğŸŒ Interface Gradio
iface = gr.Interface(
    fn=pipeline,
    inputs=gr.Image(type="pil", label="Upload a road sign image"),
    outputs=[
        gr.Textbox(label="ğŸ“· Caption"),
        gr.Textbox(label="ğŸ” Similar Signs"),
        gr.Textbox(label="ğŸ§  GPT Reasoning")
    ],
    title="ğŸ›£ï¸ Road Sign Analyzer with RAG + GPT",
    description="Upload a road sign image. The system uses BLIP (caption), CLIP+FAISS (retrieval), and GPT for reasoning."
)

if __name__ == "__main__":
    iface.launch()
