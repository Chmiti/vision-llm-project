{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtrGKqP7Ra1m"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/chmiti/vision-llm-project.git\n",
        "%cd vision-llm-project"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "ni7Qb1D2gZmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(\"data/images\", exist_ok=True)\n"
      ],
      "metadata": {
        "id": "FeG14HvSgaK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "GLQTx8Qkhk6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# D√©placer toutes les images vers data/images\n",
        "for filename in uploaded.keys():\n",
        "    shutil.move(filename, f\"data/images/{filename}\")\n"
      ],
      "metadata": {
        "id": "fL4kPkOWhpNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate bitsandbytes\n"
      ],
      "metadata": {
        "id": "1umZfyxoiXMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(token=\"hf_xxxxxxxxxxxxxxxxxxxxxxx\")\n"
      ],
      "metadata": {
        "id": "-r80_ASOjewe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "import csv\n",
        "import os\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        ")\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "def ask_mistral(prompt):\n",
        "    response = generator(prompt, max_new_tokens=150, do_sample=True, temperature=0.5)[0]['generated_text']\n",
        "    return response[len(prompt):].strip()\n",
        "\n",
        "def main(captions_file=\"captions.csv\", output_file=\"reasoning.csv\"):\n",
        "    print(\"‚úÖ D√©but inf√©rence...\")\n",
        "    with open(captions_file, newline='', encoding='utf-8') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        rows = list(reader)\n",
        "\n",
        "    with open(output_file, \"w\", newline='', encoding='utf-8') as out_f:\n",
        "        writer = csv.writer(out_f)\n",
        "        writer.writerow([\"filename\", \"caption\", \"llm_response\"])\n",
        "\n",
        "        for row in rows:\n",
        "            prompt = f\"\"\"### Instruction:\n",
        "You are a helpful assistant analyzing traffic images.\n",
        "\n",
        "### Caption:\n",
        "{row['caption']}\n",
        "\n",
        "### Question:\n",
        "What do you infer about this image?\n",
        "\n",
        "### Answer:\"\"\"\n",
        "            answer = ask_mistral(prompt)\n",
        "            print(f\"\\nüñºÔ∏è {row['filename']}\\nü§ñ {answer}\")\n",
        "            writer.writerow([row['filename'], row['caption'], answer])\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "id": "DhNF6sWTiaAu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}